{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4286,
     "status": "ok",
     "timestamp": 1608909048506,
     "user": {
      "displayName": "khalil hennara",
      "photoUrl": "",
      "userId": "12581719381754495507"
     },
     "user_tz": -120
    },
    "id": "H_T0_kjE3nez"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sn\n",
    "import re\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras \n",
    "import tensorflow_addons as tfa\n",
    "import pickle\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics_model\n",
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "f1=tfa.metrics.F1Score(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1608904160051,
     "user": {
      "displayName": "khalil hennara",
      "photoUrl": "",
      "userId": "12581719381754495507"
     },
     "user_tz": -120
    },
    "id": "zjFK4Mj83nOZ",
    "outputId": "ddd3af9b-b498-43d9-d74a-4b572e8a78a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1608909059493,
     "user": {
      "displayName": "khalil hennara",
      "photoUrl": "",
      "userId": "12581719381754495507"
     },
     "user_tz": -120
    },
    "id": "cJK7uQAo1wiW"
   },
   "outputs": [],
   "source": [
    "dic={'Positive':0,'Negative':1,'Neutral':2,'Extremely Positive':3,'Extremely Negative':4}\n",
    "Binary_dic={'Positive':0,'Negative':1,'Neutral':2,'Extremely Positive':0,'Extremely Negative':1}\n",
    "\n",
    "def replace_urls(text):\n",
    "    \"\"\"\n",
    "    This Function replace url in the text\n",
    "    \n",
    "    parameter:\n",
    "            text : is string \n",
    "    return : string\n",
    "    \"\"\"\n",
    "    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    text = re.sub(url_regex, \" \", text)\n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    This function normalizes whitespaces, removing duplicates.\n",
    "    \n",
    "    parameter:\n",
    "            text : is string\n",
    "    return : string\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"//t\",r\" \", text)\n",
    "    text = re.sub(r\"( )\\1+\",r\"\\1\", text)\n",
    "    text = re.sub(r\"(\\n)\\1+\",r\"\\1\", text)\n",
    "    text = re.sub(r\"(\\r)\\1+\",r\"\\1\", text)\n",
    "    text = re.sub(r\"(\\t)\\1+\",r\"\\1\", text)\n",
    "    return text.strip(\" \")\n",
    "\n",
    "def clean(tweet):\n",
    "    \"\"\"\n",
    "    This Function is The main Function For cleaning \n",
    "    we only used regular expretion \n",
    "    \n",
    "    parameter : \n",
    "            tweet : is string\n",
    "    return : clean string\n",
    "    \"\"\"\n",
    "\n",
    "    #remove url\n",
    "    tweet=replace_urls(tweet)\n",
    "\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"â\\x94\", \"\" , tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
    "            \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    # punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    # for p in punctuations:\n",
    "    #     tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # # ... and ..\n",
    "    tweet = tweet.replace('...', ' ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ')      \n",
    "\n",
    "    #remove mention\n",
    "    tweet=re.sub(r'@\\w+','',tweet)\n",
    "\n",
    "    #remove Hash\n",
    "    tweet=re.sub(r'#','',tweet)\n",
    "\n",
    "    #remove dupicalte whitespace\n",
    "    tweet=normalize_whitespace(tweet)\n",
    "\n",
    "    return tweet.lower()\n",
    "\n",
    "def get_ensamble_predict(models,X_test):\n",
    "  res=np.zeros((X_test.shape[0],5))\n",
    "  preds=[]\n",
    "  print('working...',end='\\t')\n",
    "  for i,model in enumerate(models):\n",
    "    if i==2:\n",
    "      pred=model.predict(X_test[:,:-10])\n",
    "    else :\n",
    "      pred=model.predict(X_test)\n",
    "    print('Done_{}'.format(i+1),end='\\t')\n",
    "    preds.append(pred)\n",
    "  res=(0.4*preds[0]+0.3*preds[1]+0.3*preds[2])\n",
    "  return res\n",
    "\n",
    "def cleaning_result(arr):\n",
    "  res=map(clean,arr)\n",
    "  return list(res)\n",
    "\n",
    "def testing_result(path):\n",
    "  print('-----Predict on 5 class-----')\n",
    "  \n",
    "  # #read_csv file \n",
    "  df=pd.read_csv(path,encoding='latin')\n",
    "  \n",
    "  # #Clean data\n",
    "  print('Clean data....')\n",
    "  df['clean_text']=df['OriginalTweet'].apply(clean)\n",
    "  df=df[df['clean_text'].isnull()==False]\n",
    "  \n",
    "  df['NomericSentiment']=df['Sentiment'].apply(lambda x: dic[x])\n",
    "  #get values of data as numpy array\n",
    "  X_test=df['clean_text'].values\n",
    "  y_test=df['NomericSentiment'].values\n",
    "\n",
    "  #convert label to one_hot vector\n",
    "  y_test=keras.utils.to_categorical(y_test,5)\n",
    "\n",
    "  #import tokenizer from file\n",
    "  with open('/content/drive/MyDrive/The_Best_model/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "  \n",
    "  print('Tokenize data...')\n",
    "  #convert string to sequnce of integer\n",
    "  X_test=tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "  # add padding to sequnce\n",
    "  X_test_set=keras.preprocessing.sequence.pad_sequences(X_test,padding='post',maxlen=70)\n",
    "\n",
    "  print(\"import model....\")\n",
    "  #import The models\n",
    "  #First one \n",
    "  model1=keras.models.load_model('/content/drive/MyDrive/The_Best_model/GRU_With_Regulization.h5')\n",
    "  model2=keras.models.load_model('/content/drive/MyDrive/The_Best_model/LSTM_with_regulization.h5')\n",
    "  model3=keras.models.load_model('/content/drive/MyDrive/The_Best_model/GRU_with_Learning_scheduler.h5')\n",
    "\n",
    "  models=[model1,model2,model3]\n",
    "\n",
    "  y_pred=get_ensamble_predict(models,X_test_set)\n",
    "\n",
    "  acc=metrics_model.accuracy_score(np.argmax(y_test,axis=1),np.argmax(y_pred,axis=1))\n",
    "  f1_score=metrics_model.f1_score(np.argmax(y_test,axis=1),np.argmax(y_pred,axis=1),average=None)\n",
    "  \n",
    "  print(\"\\nAccureacy : {:.4f}\\n f1_score : {}\".format(acc,f1_score))\n",
    "\n",
    "  print('\\n\\n-----Binary Classification------')\n",
    "  \n",
    "  print('Convert data...')\n",
    "  df['lable']=df['Sentiment'].apply(lambda x:Binary_dic[x])\n",
    "  df=df[df['lable']!=2]\n",
    "\n",
    "  y_test=df['lable'].values\n",
    "  X_test=df['clean_text'].values\n",
    "\n",
    "  y_test=y_test.astype(np.uint8)\n",
    "\n",
    "  with open('/content/drive/MyDrive/The_Best_model/Binary_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    " \n",
    "  #convert string to sequnce of integer\n",
    "  print('Tokenize data....')\n",
    "  X_test=tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "  # add padding to sequnce\n",
    "  X_test_set=keras.preprocessing.sequence.pad_sequences(X_test,padding='post',maxlen=70)\n",
    "  print('Load model ...')\n",
    "  binary_model_1=keras.models.load_model('/content/drive/MyDrive/The_Best_model/Binary_classification.h5')\n",
    "  binary_model_2=keras.models.load_model('/content/drive/MyDrive/The_Best_model/Binary_classification_2.h5')\n",
    "  print('working...',end='\\t')\n",
    "  pred1=binary_model_1.predict(X_test_set)\n",
    "  print('Done_1',end='\\t')\n",
    "  pred2=binary_model_2.predict(X_test_set)\n",
    "  print('Done_2')\n",
    "  pred=(pred1+pred2)/2\n",
    "  y_pred=pred>0.5\n",
    "\n",
    "  acc=metrics_model.accuracy_score(y_test,y_pred)\n",
    "  f1_score=metrics_model.f1_score(y_test,y_pred)\n",
    "  \n",
    "  print(\"\\nAccureacy : {:.4f}\\n f1_score : {}\\n\".format(acc,f1_score))\n",
    "\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318675,
     "status": "ok",
     "timestamp": 1608909400040,
     "user": {
      "displayName": "khalil hennara",
      "photoUrl": "",
      "userId": "12581719381754495507"
     },
     "user_tz": -120
    },
    "id": "yP3AU87M_Z9R",
    "outputId": "449276ac-41ae-4812-9eff-a5f8970e8d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Predict on 5 class-----\n",
      "Clean data....\n",
      "Tokenize data...\n",
      "import model....\n",
      "working...\tDone_1\tDone_2\tDone_3\t\n",
      "Training Accureacy : 0.9070\n",
      " f1_score : [0.90052947 0.88718818 0.90987124 0.93117656 0.92565947]\n",
      "\n",
      "\n",
      "-----Binary Classification------\n",
      "Convert data...\n",
      "Tokenize data....\n",
      "Load model ...\n",
      "working...\tDone_1\tDone_2\n",
      "\n",
      "Training Accureacy : 0.9547\n",
      " f1_score : 0.9511045084614391\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_result('/content/drive/MyDrive/Covid_19_tweets_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEcs3HkMPbH4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNnGFir0gX05oggAp2oBy9",
   "collapsed_sections": [],
   "name": "evaluate_on_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
